{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3 - Word Frequencies by Party.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8bIybEXmDhu"
      },
      "source": [
        "In this notebook, we use the tweet data assembled as in Notebook 2 - Data Preprocessing, and combine these into a dictionary of word counts for tweets by democrats, and a dictionary of word counts for tweets by republicans. We then look at the most frequent words tweeted by each party, to give us insight into patterns in what each party is tweeting about."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW0Yse4Di0of"
      },
      "source": [
        "# Setup\n",
        "\n",
        "In this section, we repeat the code from Notebook 2 - Data Preprocessing, which sets up the data structures that we use for analysis. The code here is included without explanation, for the documentation for this code, refer back to Notebook 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uU5MJ3lfg4C"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of dates for which data was gathered. This is used in the file names.\n",
        "dates = [\"11\" + str(day) for day in range(15, 27)]\n",
        "\n",
        "# Location of the csv files containing tweets\n",
        "location = \"https://raw.githubusercontent.com/lynn0032/CourseProject/main/data_files/\"\n",
        "\n",
        "column_names = [\"Index\", \"Twitter Handle\", \"Created At\", \"ID\", \"Tweet Text\", \"State\", \"Branch\", \"Last Name\", \"First Name\"]\n",
        "tweets_df = pd.DataFrame(columns = column_names)\n",
        "\n",
        "# Load tweet data from each day, and combine them in the dataframe tweets_df\n",
        "for d in dates:\n",
        "  file_name = \"all_tweets_\" + d + \".csv\"\n",
        "  day_df = pd.read_csv(location + file_name)\n",
        "  tweets_df = pd.concat([tweets_df, day_df], axis = 0)\n",
        "\n",
        "#Drop the extra Index column\n",
        "tweets_df = tweets_df.drop(columns = [\"Index\"])"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfXHyw-SjA4c"
      },
      "source": [
        "#Remove duplicate tweets\n",
        "tweets_df = tweets_df.drop_duplicates()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okHvRBU2j9Jw",
        "outputId": "52d8494f-638c-4968-d8f6-aa95cf0fe547"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxo5IK6CkBCb"
      },
      "source": [
        "def remove_punctuation(word:str) -> str:\n",
        "  \"\"\"Takes a word, and returns the word with characters \n",
        "  except for lowercase letters and @ and # removed\"\"\"\n",
        "  new_word = \"\"\n",
        "  allowed = \"abcdefghijklmnopqrstuvwxyz@#\"\n",
        "  for char in word:\n",
        "    if char in allowed:\n",
        "      new_word += char\n",
        "  return new_word\n",
        "\n",
        "stop_words = stopwords.words('english')     #stop words from nltk\n",
        "\n",
        "def get_words(text_str) -> list:\n",
        "  \"\"\"Takes a tweet, splits it into words based on spaces,\n",
        "  removes extra punctuation from the words, and then\n",
        "  removes stop words\"\"\"\n",
        "  words = [remove_punctuation(w.lower()) for w in text_str.split(\" \")]\n",
        "  filtered_words = [w for w in words if w not in stop_words]\n",
        "  return [word for word in filtered_words if word != \"\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu3ICkZJkGEN"
      },
      "source": [
        "def retweet(words:list) -> bool:\n",
        "  \"\"\"Takes a list of words from a tweet (as returned by get_words), \n",
        "  and returns True if 'rt' is one of the words (which means that \n",
        "  the tweet is a retweet)\"\"\"\n",
        "  return ('rt' in words)\n",
        "\n",
        "def tweet_at(words:list) -> list:\n",
        "  \"\"\"Takes a list of words from a tweet (as returned by get_words),\n",
        "  and returns a list of handles that are tweeted at in the tweet\"\"\"\n",
        "  at = []\n",
        "  for word in words:\n",
        "    if word[0] == '@':\n",
        "      at.append(word[1:])\n",
        "  return at\n",
        "\n",
        "def hash_tags(words:list) -> list:\n",
        "  \"\"\"Takes a list of words from a tweet (as returned by get_words),\n",
        "  and returns a list of all hashtags included in the tweet\"\"\"\n",
        "  tags = []\n",
        "  for word in words:\n",
        "    if word[0] == '#':\n",
        "      tags.append(word[1:])\n",
        "  return tags"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZp8sZHDkM6_"
      },
      "source": [
        "ps = PorterStemmer()    # Word stemmer from nltk\n",
        "\n",
        "def word_counts(words:list) -> dict:\n",
        "  \"\"\"Takes a list of words from a tweet (as returned by get_words),\n",
        "  and returns a dictionary of the counts of words from the tweet\"\"\"\n",
        "  counts_dict = {}\n",
        "  for word in words:\n",
        "    if word[0] != '@' and word[0] != '#' and word != 'rt' and 'http' not in word:\n",
        "      word = ps.stem(word)\n",
        "      if word in counts_dict:\n",
        "        counts_dict[word] += 1\n",
        "      else:\n",
        "        counts_dict[word] = 1\n",
        "  return counts_dict"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9uRkdWEkSRv"
      },
      "source": [
        "def parse_tweet(text:str) -> dict:\n",
        "  words = get_words(text)\n",
        "  return {\"counts\": word_counts(words), \"retweet\": retweet(words), \"at\": tweet_at(words), \"tags\": hash_tags(words)}\n",
        "  \n",
        "tweet_dict = {}\n",
        "for tweet in tweets_df['Tweet Text']:\n",
        "  tweet = str(tweet)\n",
        "  tweet_dict[tweet] = parse_tweet(tweet)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23DYXzL9kbPm"
      },
      "source": [
        "handle_parties_df = tweets_df[['Twitter Handle', 'Party']]\n",
        "handle_parties_df.drop_duplicates()\n",
        "parties = {}\n",
        "for index, row in handle_parties_df.iterrows():\n",
        "  handle = row['Twitter Handle']\n",
        "  parties[handle] = row['Party']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YJ3UqtG7XfZ"
      },
      "source": [
        "def handle_word_counts(handle):\n",
        "  handle_df = tweets_df[tweets_df['Twitter Handle'] == handle]\n",
        "  all_counts = {}\n",
        "  for tweet in handle_df['Tweet Text']:\n",
        "    tweet = str(tweet)\n",
        "    for word in tweet_dict[tweet]['counts']:\n",
        "      if word in all_counts:\n",
        "        all_counts[word] += tweet_dict[tweet]['counts'][word]\n",
        "      else:\n",
        "        all_counts[word] = tweet_dict[tweet]['counts'][word]\n",
        "  return all_counts"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxRISbRO7YF0"
      },
      "source": [
        "handles = list(set(tweets_df['Twitter Handle'].unique()))\n",
        "\n",
        "handle_counts_dict = {}\n",
        "for handle in handles:\n",
        "  handle_counts_dict[handle] = handle_word_counts(handle)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yTxtLXHmUaC"
      },
      "source": [
        "# Word Frequencies by Party\n",
        "\n",
        "Now, we are ready to start computing word frequencies by party.\n",
        "\n",
        "In the next code cell, we build dictionaries with word counts for tweets by democrats and tweets by republicans. This is done by iterating through tweets from the dataframe tweets_df, using the word counts assembled in the dictionary tweet_dict, and accumulating these accounts in the dictionary for the correct party. \n",
        "\n",
        "Note that we are already iterating through the dataframe tweets_df, so we can determine the party directly from the dataframe, rather than looking it up in the dictionary parties."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgHVQ73Tmqr2"
      },
      "source": [
        "dem_counts = {}\n",
        "rep_counts = {}\n",
        "\n",
        "for index, row in tweets_df.iterrows():\n",
        "  tweet = str(row['Tweet Text'])\n",
        "  words = tweet_dict[tweet]['counts']\n",
        "  for word in words:\n",
        "    if str(row['Party']) == 'D':    #if the party for the current tweet is D, accumulate in dem_counts\n",
        "      if word in dem_counts:\n",
        "        dem_counts[word] += words[word]\n",
        "      else:\n",
        "        dem_counts[word] = words[word]\n",
        "    if str(row['Party']) == \"R\":    #if the party for the current tweet is R, accumulate in rep_counts\n",
        "      if word in rep_counts:\n",
        "        rep_counts[word] += words[word]\n",
        "      else:\n",
        "        rep_counts[word] = words[word]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ysn5ACNmqGi"
      },
      "source": [
        "The code above produces two dictionaries with raw word counts. In the next block of code, we take these dictionaries and normalize by the total number of words, producing word frequencies by political party."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdDMrSJom5Kj"
      },
      "source": [
        "dem_total_words = sum(dem_counts.values())\n",
        "dem_freq = {}\n",
        "for word in dem_counts:\n",
        "  dem_freq[word] = dem_counts[word] / dem_total_words\n",
        "\n",
        "rep_total_words = sum(rep_counts.values())\n",
        "rep_freq = {}\n",
        "for word in rep_counts:\n",
        "  rep_freq[word] = rep_counts[word] / rep_total_words"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwb34poNnNWw"
      },
      "source": [
        "Now, we sort the words in descending order of frequency, in order the find the top ten words used by each political party.\n",
        "\n",
        "We first find the top ten words used by democrats, along with their frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz__IaTrnMKf",
        "outputId": "85734d5b-8e24-4b22-c319-55d32e5c8fbd"
      },
      "source": [
        "dem_pairs = list(dem_freq.items())\n",
        "dem_pairs.sort(reverse = True, key = lambda x:x[1])\n",
        "for pair in dem_pairs[:10]:\n",
        "  print(pair[0], pair[1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "infrastructur 0.011572551424271141\n",
            "act 0.010243603322370254\n",
            "today 0.009654941747031607\n",
            "amp 0.009425274541501756\n",
            "invest 0.008651541334522549\n",
            "famili 0.007503205306873293\n",
            "bipartisan 0.007427392831261498\n",
            "bill 0.007182117174870394\n",
            "thank 0.006901165059367858\n",
            "work 0.006381626623557612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu4RTiBnnWKP"
      },
      "source": [
        "Next, we find the top ten words used by republicans, along with their frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d23-S2u-nXPO",
        "outputId": "4612074a-9e0d-42e7-d739-60a5aabd4c94"
      },
      "source": [
        "rep_pairs = list(rep_freq.items())\n",
        "rep_pairs.sort(reverse = True, key = lambda x:x[1])\n",
        "for pair in rep_pairs[:10]:\n",
        "  print(pair[0], pair[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "biden 0.014706966558444218\n",
            "american 0.009330431624620106\n",
            "democrat 0.008068898288192102\n",
            "amp 0.007853181548846188\n",
            "today 0.006990314591462532\n",
            "thank 0.006725447202645397\n",
            "bill 0.006569803479319864\n",
            "spend 0.005971803910753343\n",
            "presid 0.005431146766569913\n",
            "year 0.0053656125672749515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wApaX5vKpof5"
      },
      "source": [
        "Just from these most common words, we can make some interesting observations.\n",
        "\n",
        "The top two most frequent (stemmed) words for democrats are \"infrastructur\" and \"act\", which makes sense since the democrats have been focusing on passing the Infrastructure Investment and Jobs Act, which was recently passed on 11/15, during the timeframe of data collection. The other most frequent words also seem to connect to passing this act.\n",
        "\n",
        "Among the top three most frequent (stemmed) words are \"biden\" and \"democrat\", and \"presid\" is lower on the list. This shows the republican focus on the opposing party."
      ]
    }
  ]
}