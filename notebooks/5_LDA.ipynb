{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5 - LDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuwvTFAx5CWJ"
      },
      "source": [
        "In this notebook, we use LDA to find the natural topic groupings among Twitter accounts. We then look at the distribution of democrats and republicans in these groupings, to see if LDA grouped the accounts along party lines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW0Yse4Di0of"
      },
      "source": [
        "# Setup\n",
        "\n",
        "In this section, we repeat the code from Notebook 2 - Data Preprocessing, which sets up the data structures that we use for analysis. The code here is included without explanation, for the documentation for this code, refer back to Notebook 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uU5MJ3lfg4C"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of dates for which data was gathered. This is used in the file names.\n",
        "dates = [\"11\" + str(day) for day in range(15, 27)]\n",
        "\n",
        "# Location of the csv files containing tweets\n",
        "location = \"https://raw.githubusercontent.com/lynn0032/CourseProject/main/data_files/\"\n",
        "\n",
        "column_names = [\"Index\", \"Twitter Handle\", \"Created At\", \"ID\", \"Tweet Text\", \"State\", \"Branch\", \"Last Name\", \"First Name\"]\n",
        "tweets_df = pd.DataFrame(columns = column_names)\n",
        "\n",
        "# Load tweet data from each day, and combine them in the dataframe tweets_df\n",
        "for d in dates:\n",
        "  file_name = \"all_tweets_\" + d + \".csv\"\n",
        "  day_df = pd.read_csv(location + file_name)\n",
        "  tweets_df = pd.concat([tweets_df, day_df], axis = 0)\n",
        "\n",
        "#Drop the extra Index column\n",
        "tweets_df = tweets_df.drop(columns = [\"Index\"])"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfXHyw-SjA4c"
      },
      "source": [
        "#Remove duplicate tweets\n",
        "tweets_df = tweets_df.drop_duplicates()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okHvRBU2j9Jw",
        "outputId": "7b1967d8-b546-4a2e-bbc1-00ae13215713"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxo5IK6CkBCb"
      },
      "source": [
        "def remove_punctuation(word:str) -> str:\n",
        "  \"\"\"Takes a word, and returns the word with characters \n",
        "  except for lowercase letters and @ and # removed\"\"\"\n",
        "  new_word = \"\"\n",
        "  allowed = \"abcdefghijklmnopqrstuvwxyz@#\"\n",
        "  for char in word:\n",
        "    if char in allowed:\n",
        "      new_word += char\n",
        "  return new_word\n",
        "\n",
        "stop_words = stopwords.words('english')     #stop words from nltk\n",
        "\n",
        "def get_words(text_str) -> list:\n",
        "  \"\"\"Takes a tweet, splits it into words based on spaces,\n",
        "  removes extra punctuation from the words, and then\n",
        "  removes stop words\"\"\"\n",
        "  words = [remove_punctuation(w.lower()) for w in text_str.split(\" \")]\n",
        "  filtered_words = [w for w in words if w not in stop_words]\n",
        "  return [word for word in filtered_words if word != \"\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu3ICkZJkGEN"
      },
      "source": [
        "def retweet(words:list) -> bool:\n",
        "  \"\"\"Takes a list of words from a tweet (as returned by get_words), \n",
        "  and returns True if 'rt' is one of the words (which means that \n",
        "  the tweet is a retweet)\"\"\"\n",
        "  return ('rt' in words)\n",
        "\n",
        "def tweet_at(words:list) -> list:\n",
        "  \"\"\"Takes a list of words from a tweet (as returned by get_words),\n",
        "  and returns a list of handles that are tweeted at in the tweet\"\"\"\n",
        "  at = []\n",
        "  for word in words:\n",
        "    if word[0] == '@':\n",
        "      at.append(word[1:])\n",
        "  return at\n",
        "\n",
        "def hash_tags(words:list) -> list:\n",
        "  \"\"\"Takes a list of words from a tweet (as returned by get_words),\n",
        "  and returns a list of all hashtags included in the tweet\"\"\"\n",
        "  tags = []\n",
        "  for word in words:\n",
        "    if word[0] == '#':\n",
        "      tags.append(word[1:])\n",
        "  return tags"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZp8sZHDkM6_"
      },
      "source": [
        "ps = PorterStemmer()    # Word stemmer from nltk\n",
        "\n",
        "def word_counts(words:list) -> dict:\n",
        "  \"\"\"Takes a list of words from a tweet (as returned by get_words),\n",
        "  and returns a dictionary of the counts of words from the tweet\"\"\"\n",
        "  counts_dict = {}\n",
        "  for word in words:\n",
        "    if word[0] != '@' and word[0] != '#' and word != 'rt' and 'http' not in word:\n",
        "      word = ps.stem(word)\n",
        "      if word in counts_dict:\n",
        "        counts_dict[word] += 1\n",
        "      else:\n",
        "        counts_dict[word] = 1\n",
        "  return counts_dict"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9uRkdWEkSRv"
      },
      "source": [
        "def parse_tweet(text:str) -> dict:\n",
        "  words = get_words(text)\n",
        "  return {\"counts\": word_counts(words), \"retweet\": retweet(words), \"at\": tweet_at(words), \"tags\": hash_tags(words)}\n",
        "  \n",
        "tweet_dict = {}\n",
        "for tweet in tweets_df['Tweet Text']:\n",
        "  tweet = str(tweet)\n",
        "  tweet_dict[tweet] = parse_tweet(tweet)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23DYXzL9kbPm"
      },
      "source": [
        "handle_parties_df = tweets_df[['Twitter Handle', 'Party']]\n",
        "handle_parties_df.drop_duplicates()\n",
        "parties = {}\n",
        "for index, row in handle_parties_df.iterrows():\n",
        "  handle = row['Twitter Handle']\n",
        "  parties[handle] = row['Party']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x30FnF588mXI"
      },
      "source": [
        "def handle_word_counts(handle):\n",
        "  handle_df = tweets_df[tweets_df['Twitter Handle'] == handle]\n",
        "  all_counts = {}\n",
        "  for tweet in handle_df['Tweet Text']:\n",
        "    tweet = str(tweet)\n",
        "    for word in tweet_dict[tweet]['counts']:\n",
        "      if word in all_counts:\n",
        "        all_counts[word] += tweet_dict[tweet]['counts'][word]\n",
        "      else:\n",
        "        all_counts[word] = tweet_dict[tweet]['counts'][word]\n",
        "  return all_counts"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THTJ9IU28nyv"
      },
      "source": [
        "handles = list(set(tweets_df['Twitter Handle'].unique()))\n",
        "\n",
        "handle_counts_dict = {}\n",
        "for handle in handles:\n",
        "  handle_counts_dict[handle] = handle_word_counts(handle)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG5Kmm-o5Ve5"
      },
      "source": [
        "# Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "We will build our LDA model using the library gensim. In order to do this, we need to encode words as numbers. This is done in the following code, by creating a set of all words from the dataset of tweets (already tokenized and stemmed), and then creating dictionaries that can be used to look up the id (number) for a word, and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6SrI_TZ5cRQ"
      },
      "source": [
        "# Construct a set containing all words\n",
        "all_words = set()\n",
        "for tweet in tweet_dict:\n",
        "  words = tweet_dict[tweet]['counts'].keys()\n",
        "  for w in words:\n",
        "    all_words.add(w)\n",
        "\n",
        "# construct dictionaries id_to_word\n",
        "# id_to_word can be used to look up the word by its id, with id_to_word[id]\n",
        "# word_to_id can be used to look up the id by its word, with word_to_id[word]\n",
        "id_to_word = {}\n",
        "word_to_id = {}\n",
        "id = 0\n",
        "for word in all_words:\n",
        "  id_to_word[id] = word\n",
        "  word_to_id[word] = id\n",
        "  id += 1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM8v3-kI5ezy"
      },
      "source": [
        "Next we build the corpus, which is the set of documents. In this case, each document is the combined word count of all tweets from a handle. So, each document corresponds to one Twitter account.\n",
        "\n",
        "For gensim's LDA model, the corpus is stored as a list of documents, where each document is a list of tuples. Each tuple consists of the id of a word, and the count of that word in the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_SBusQY5gt-"
      },
      "source": [
        "handles = tweets_df['Twitter Handle'].unique()\n",
        "\n",
        "doc_handles = {}\n",
        "\n",
        "corpus = []\n",
        "doc_num = 0\n",
        "for handle in handle_counts_dict:\n",
        "  doc_handles[doc_num] = handle\n",
        "  doc_num += 1\n",
        "  doc = []\n",
        "  for word in handle_counts_dict[handle]:\n",
        "    doc.append((word_to_id[word], handle_counts_dict[handle][word]))\n",
        "  corpus.append(doc)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7QYc0VY5mA_"
      },
      "source": [
        "Now, we train the LDA model, using the library gensim. We do this for two topics, which we will then compare with the parties for each tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U_YidVF5k21"
      },
      "source": [
        "import gensim\n",
        "\n",
        "num_topics = 2\n",
        "\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id_to_word, num_topics=num_topics)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEuON7mX5rde"
      },
      "source": [
        "The LDA model takes documents, and returns the estimated mixing values for each topic. For instance, this might be the list [(0, .25), (1, .75)], which would indicate that 25% of the document is drawn from topic 0, while 75% of the document is drawn from topic 1. The following function takes these results, and selects and returns the topic with the largest proportion, identifying this as the main topic of the document. For the example given, this would be topic 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGXvZzRk5t2D"
      },
      "source": [
        "def main_topic(topics):\n",
        "  best_percent = 0\n",
        "  for pair in topics:\n",
        "    if pair[1] > best_percent:\n",
        "      best_percent = pair[1]\n",
        "      best_topic = pair[0]\n",
        "  return best_topic"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muCKdTmV-jJj"
      },
      "source": [
        "In the code below, we iterate through all documents in the corpus. For each document, we find the political party of the account owner, and identify the main topic of the document (as discovered by the LDA model).\n",
        "\n",
        "For each party, we find the number of documents in each topic discovered by the LDA model, to determine if the split into topics aligns with political parties."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3JWyZ54-jc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d30a111-9845-47d6-9a9a-e763c8e8eb4d"
      },
      "source": [
        "topic_dist = []\n",
        "dem_topics = [0 for index in range(num_topics)]\n",
        "rep_topics = [0 for index in range(num_topics)]\n",
        "for index in range(len(corpus)):\n",
        "  doc = corpus[index]\n",
        "  handle = doc_handles[index]\n",
        "  party = parties[handle]\n",
        "  topics = lda_model[doc]\n",
        "  best_topic = main_topic(topics)\n",
        "  topic_dist.append((handle, party, topics, best_topic))\n",
        "  if party == \"D\":\n",
        "    dem_topics[best_topic] += 1\n",
        "  if party == \"R\":\n",
        "    rep_topics[best_topic] += 1\n",
        "\n",
        "print(\"Democrat's topics:\", dem_topics)\n",
        "print(\"Republican's topics:\", rep_topics)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Democrat's topics: [170, 104]\n",
            "Republican's topics: [124, 137]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_9iVzlV-mNW"
      },
      "source": [
        "From these results (for an example run - results will vary), we see that Topic 0 has 203 republicans and 58 democrats, and Topic 1 has 216 democrats and 58 republicans. So, we see that the topics discovered by the LDA model align somewhat with political parties, though not completely.\n",
        "\n",
        "Finally, we print the twenty words with the highest frequency in each of the topics generated by the LDA model, to see if this provides any insight into what these topics are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4wxlOwG-le3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caafc02d-6826-4fba-bd6f-a2d8f6fce653"
      },
      "source": [
        "lda_model.print_topics(num_words=20)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.009*\"today\" + 0.008*\"amp\" + 0.008*\"act\" + 0.008*\"infrastructur\" + 0.007*\"famili\" + 0.007*\"thank\" + 0.007*\"biden\" + 0.007*\"american\" + 0.006*\"bill\" + 0.006*\"join\" + 0.006*\"hous\" + 0.005*\"invest\" + 0.005*\"presid\" + 0.005*\"veteran\" + 0.005*\"back\" + 0.005*\"us\" + 0.004*\"pass\" + 0.004*\"work\" + 0.004*\"year\" + 0.004*\"build\"'),\n",
              " (1,\n",
              "  '0.009*\"biden\" + 0.009*\"amp\" + 0.008*\"american\" + 0.007*\"today\" + 0.007*\"bill\" + 0.007*\"infrastructur\" + 0.006*\"bipartisan\" + 0.006*\"thank\" + 0.006*\"year\" + 0.006*\"work\" + 0.006*\"act\" + 0.005*\"day\" + 0.005*\"democrat\" + 0.005*\"us\" + 0.005*\"invest\" + 0.005*\"back\" + 0.005*\"hous\" + 0.004*\"im\" + 0.004*\"great\" + 0.004*\"tax\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qABJm7il-t7Q"
      },
      "source": [
        "Unfortunately, it's difficult to clearly understand the topics generated from these lists of words. Both topics seem to have similar words with high frequency, though with slightly different weights and rankings."
      ]
    }
  ]
}